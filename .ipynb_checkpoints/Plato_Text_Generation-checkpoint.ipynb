{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QINqPZH6FzB3",
    "outputId": "5ec53663-fa3b-471a-b4cc-f30f117364bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l\r",
      "\u001b[K     |▊                               | 10kB 27.9MB/s eta 0:00:01\r",
      "\u001b[K     |█▍                              | 20kB 2.1MB/s eta 0:00:01\r",
      "\u001b[K     |██                              | 30kB 2.5MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 40kB 3.0MB/s eta 0:00:01\r",
      "\u001b[K     |███▍                            | 51kB 2.3MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 61kB 2.6MB/s eta 0:00:01\r",
      "\u001b[K     |████▊                           | 71kB 2.9MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 81kB 3.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████                          | 92kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 102kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████▍                        | 112kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 122kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████▊                       | 133kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▍                      | 143kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 153kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▊                     | 163kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▍                    | 174kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████                    | 184kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 194kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▍                  | 204kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▏                 | 215kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 225kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▌                | 235kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 245kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▉               | 256kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▌              | 266kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▏             | 276kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████▉             | 286kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▌            | 296kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▏           | 307kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▉           | 317kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 327kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▏         | 337kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▉         | 348kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 358kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▏       | 368kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 378kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 389kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▏     | 399kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████▉     | 409kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 419kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▎   | 430kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████   | 440kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 450kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▎ | 460kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████ | 471kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 481kB 3.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 491kB 3.3MB/s \n",
      "\u001b[?25h  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[K     |████████████████████████████████| 471kB 3.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 174kB 7.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.6MB 6.8MB/s \n",
      "\u001b[K     |████████████████████████████████| 3.6MB 35.7MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.1MB 55.1MB/s \n",
      "\u001b[K     |████████████████████████████████| 71kB 7.3MB/s \n",
      "\u001b[K     |████████████████████████████████| 348kB 75.0MB/s \n",
      "\u001b[K     |████████████████████████████████| 1.3MB 49.2MB/s \n",
      "\u001b[K     |████████████████████████████████| 890kB 56.9MB/s \n",
      "\u001b[K     |████████████████████████████████| 2.9MB 56.6MB/s \n",
      "\u001b[?25h  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for gin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "grpc://10.32.169.210:8470\n"
     ]
    }
   ],
   "source": [
    "# Install dependencies\n",
    "!pip install --upgrade -q jax\n",
    "!pip install --upgrade -q jaxlib\n",
    "!pip install --upgrade -q trax\n",
    "!pip install --upgrade -q sentencepiece\n",
    "!pip install --upgrade -q gin \n",
    "\n",
    "# Make sure the Colab Runtime is set to Accelerator: TPU.\n",
    "import requests\n",
    "import os\n",
    "if 'TPU_DRIVER_MODE' not in globals():\n",
    "  url = 'http://' + os.environ['COLAB_TPU_ADDR'].split(':')[0] + ':8475/requestversion/tpu_driver0.1-dev20191206'\n",
    "  resp = requests.post(url)\n",
    "  TPU_DRIVER_MODE = 1\n",
    "\n",
    "# The following is required to use TPU Driver as JAX's backend.\n",
    "from jax.config import config\n",
    "config.FLAGS.jax_xla_backend = \"tpu_driver\"\n",
    "config.FLAGS.jax_backend_target = \"grpc://\" + os.environ['COLAB_TPU_ADDR']\n",
    "print(config.FLAGS.jax_backend_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "pv0PuxC3F-jn"
   },
   "outputs": [],
   "source": [
    "import gin\n",
    "import os\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "\n",
    "# Zipping and downloading files\n",
    "from google.colab import files\n",
    "import shutil\n",
    "\n",
    "# Trax\n",
    "import jax\n",
    "import trax\n",
    "from trax.data import inputs\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# NLP Vocab Generation\n",
    "import sentencepiece as spm\n",
    "\n",
    "# TensorFlow\n",
    "from tensorflow.compat.v1.io.gfile import GFile\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gv4xf_1NIBpy",
    "outputId": "7add115d-02b5-42c7-dc04-6aecf8888cb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from http://www.gutenberg.org/cache/epub/1497/pg1497.txt\n",
      "1245184/1239081 [==============================] - 1s 1us/step\n"
     ]
    }
   ],
   "source": [
    "# Download `The Republic` by Plato text\n",
    "FILENAME = '.'.join(['the_republic', 'txt'])\n",
    "URL = 'http://www.gutenberg.org/cache/epub/1497/pg1497.txt'\n",
    "tf.keras.utils.get_file(FILENAME, URL, cache_dir='.')\n",
    "TEXT_PATH = os.path.join('datasets', FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yRnIoCBkNXyd"
   },
   "outputs": [],
   "source": [
    "# Use only novel text\n",
    "with GFile(TEXT_PATH) as f:\n",
    "    text = f.read()\n",
    "\n",
    "start = text.rfind('INTRODUCTION AND ANALYSIS')\n",
    "start = text.find('The Republic', start + 1)\n",
    "end = text.rfind('End of the Project Gutenberg EBook of The Republic, by Plato')\n",
    "text = text[start:end].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_l9Znw2bS-Pg",
    "outputId": "10b8d851-a034-4a52-ad46-138e6212086f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train a BPE model on the text\n",
    "spm.SentencePieceTrainer.train('--input=datasets/the_republic.txt \\\n",
    "                                --model_prefix=cp.320 \\\n",
    "                                --vocab_size=320 \\\n",
    "                                --model_type=bpe') \n",
    "# Load BPE vocabulary\n",
    "TOKENIZER = spm.SentencePieceProcessor() \n",
    "TOKENIZER.load('cp.320.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OQwhEqJzS-I2",
    "outputId": "62d65742-9a5f-4bc0-84cb-fa36cbe086d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 512874\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "IDS = TOKENIZER.EncodeAsIds(text)\n",
    "IDS = np.asarray(IDS, dtype=np.int32)\n",
    "PAD_AMOUNT = 512 * 1024 - len(IDS)\n",
    "print(\"Number of tokens:\", IDS.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bdKnx2FyS-AO",
    "outputId": "4a5daa60-ed13-456e-b764-530494f135b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(device count, tokens per device) =  (8, 524288)\n"
     ]
    }
   ],
   "source": [
    "# Set up the data pipeline.\n",
    "def my_inputs(n_devices):\n",
    "  while True:\n",
    "    inputs = []\n",
    "    mask = []\n",
    "    pad_amounts = np.random.choice(PAD_AMOUNT, n_devices)\n",
    "    for i in range(n_devices):\n",
    "      inputs.append(np.pad(IDS, (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]), # Pad IDS by different amount for each device\n",
    "                            mode='constant'))\n",
    "      mask.append(np.pad(np.ones_like(IDS, dtype=np.float32),\n",
    "                          (pad_amounts[i], PAD_AMOUNT - pad_amounts[i]),\n",
    "                          mode='constant'))\n",
    "    inputs = np.stack(inputs)\n",
    "    mask = np.stack(mask)\n",
    "    yield (inputs, inputs, mask)\n",
    "\n",
    "print(\"(device count, tokens per device) = \",\n",
    "      next(my_inputs(trax.fastmath.device_count()))[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "-NeFGlGGS959"
   },
   "outputs": [],
   "source": [
    "# Configure hyperparameters.\n",
    "gin.parse_config(\"\"\"\n",
    "import trax.layers\n",
    "import trax.models\n",
    "import trax.optimizers\n",
    "import trax.data.inputs\n",
    "import trax.supervised.trainer_lib\n",
    "\n",
    "# Parameters that will vary between experiments:\n",
    "# ==============================================================================\n",
    "train.model = @trax.models.ReformerLM\n",
    "# Model will have 6 layers, alternating between the LSH attention\n",
    "# and local attention within a certain context window.\n",
    "n_layers = 6\n",
    "attn_type = [\n",
    "  @trax.layers.SelfAttention,\n",
    "  @LSHSelfAttention,  \n",
    "  @trax.layers.SelfAttention,\n",
    "  @LSHSelfAttention,\n",
    "  @trax.layers.SelfAttention,\n",
    "  @LSHSelfAttention,\n",
    "  ]\n",
    "share_qk = False  # LSH attention ignores this flag and always shares q & k\n",
    "n_heads = 2\n",
    "attn_kv = 64\n",
    "dropout = 0.05\n",
    "n_tokens = 524288\n",
    "\n",
    "# Parameters for multifactor:\n",
    "# ==============================================================================\n",
    "multifactor.constant = 0.01\n",
    "multifactor.factors = 'constant * linear_warmup * cosine_decay'\n",
    "multifactor.warmup_steps = 100\n",
    "multifactor.steps_per_cycle = 900\n",
    "\n",
    "# Parameters for Adam:\n",
    "# ==============================================================================\n",
    "Adam.weight_decay_rate=0.0\n",
    "Adam.b1 = 0.86\n",
    "Adam.b2 = 0.92\n",
    "Adam.eps = 1e-9\n",
    "\n",
    "# Parameters for SelfAttention:\n",
    "# ==============================================================================\n",
    "trax.layers.SelfAttention.attention_dropout = 0.05\n",
    "trax.layers.SelfAttention.chunk_len = 64\n",
    "trax.layers.SelfAttention.n_chunks_before = 1\n",
    "trax.layers.SelfAttention.n_parallel_heads = 1\n",
    "\n",
    "# Parameters for LSHSelfAttention:\n",
    "# ==============================================================================\n",
    "LSHSelfAttention.attention_dropout = 0.0\n",
    "LSHSelfAttention.chunk_len = 64\n",
    "LSHSelfAttention.n_buckets = [64, 128]\n",
    "LSHSelfAttention.n_chunks_after = 0\n",
    "LSHSelfAttention.n_chunks_before = 1\n",
    "LSHSelfAttention.n_hashes = 1\n",
    "LSHSelfAttention.n_parallel_heads = 1\n",
    "LSHSelfAttention.predict_drop_len = 128\n",
    "LSHSelfAttention.predict_mem_len = 1024\n",
    "\n",
    "# Parameters for ReformerLM:\n",
    "# ==============================================================================\n",
    "ReformerLM.attention_type = %attn_type\n",
    "ReformerLM.d_attention_key = %attn_kv\n",
    "ReformerLM.d_attention_value = %attn_kv\n",
    "ReformerLM.d_model = 256\n",
    "ReformerLM.d_ff = 512\n",
    "ReformerLM.dropout = %dropout\n",
    "ReformerLM.ff_activation = @trax.layers.Relu\n",
    "ReformerLM.max_len = %n_tokens\n",
    "ReformerLM.mode = 'train'\n",
    "ReformerLM.n_heads = %n_heads\n",
    "ReformerLM.n_layers = %n_layers\n",
    "ReformerLM.vocab_size = 320\n",
    "ReformerLM.axial_pos_shape = (512, 1024)\n",
    "ReformerLM.d_axial_pos_embs= (64, 192)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "X8ay-6VxS9xw"
   },
   "outputs": [],
   "source": [
    "# Trainer.\n",
    "output_dir = os.path.expanduser('model')\n",
    "!rm -f ~/model/model.pkl.gz  # Remove old model\n",
    "\n",
    "trainer = trax.supervised.Trainer(\n",
    "    model=trax.models.ReformerLM,\n",
    "    loss_fn=trax.layers.CrossEntropyLoss(),\n",
    "    optimizer=trax.optimizers.Adam,\n",
    "    lr_schedule=trax.lr.multifactor(),\n",
    "    inputs=trax.data.inputs.Inputs(my_inputs),\n",
    "    output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf0HITY7Wz8C"
   },
   "outputs": [],
   "source": [
    "# Train Model\n",
    "for _ in range(50):\n",
    "  trainer.train_epoch(n_steps=100, n_eval_steps=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 37
    },
    "id": "JEikLctyte-m",
    "outputId": "d859f5ee-71f9-458b-951f-8aacd3784c43"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/text_generation.zip'"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Zip directory contents\n",
    "shutil.make_archive(\"project\", \"zip\", \".\")\n",
    "\n",
    "# Download zipped directory\n",
    "files.download('project.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "FLebeiAFrItK"
   },
   "outputs": [],
   "source": [
    "# In the Reformer paper, increasing the number of hashing rounds helps with quality. \n",
    "# The number of hashing rounds at can be increased at evaluation time only.\n",
    "gin.parse_config(\"\"\"LSHSelfAttention.n_hashes = 8\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "thdiRvGDrJhy",
    "outputId": "22fc7d95-9535-49dd-94a4-db715d354c83"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'The Republic of Plato. The man is remost highest philosophy of Herodic has been said to have recognised an ancient to the authority of Hellas. The greatest of all knowing that he doubted at Socrates and Plato, is the practicabil'"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the trained Reformer in 'predict' mode\n",
    "model = trax.models.ReformerLM(mode='predict')\n",
    "model.init_from_file(os.path.join(output_dir,'model.pkl.gz'),\n",
    "                     weights_only=True)\n",
    "\n",
    "# Sample from ReformerLM\n",
    "output_token_ids = trax.supervised.decoding.autoregressive_sample(\n",
    "    model, temperature=0.2)\n",
    "\n",
    "# Decode token IDs\n",
    "# Reformer outputed a batch with one item so access it using [0]\n",
    "# tolist() converts from int64 to int, the type SentencePiece expects\n",
    "TOKENIZER.DecodeIds(output_token_ids[0].tolist())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "name": "Plato Text Generation.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
